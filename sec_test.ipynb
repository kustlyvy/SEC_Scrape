{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sec_test.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNl869maOEfg3bkfiOXuRK5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/reckoning-machines/sec_test/blob/master/sec_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MxREunso0REA",
        "colab_type": "code",
        "outputId": "2c9e3cc5-df63-4085-8005-85e8cdfb79b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# why use R here?  \n",
        "# edgarWebR pulls sections really well\n",
        "# do i want to find a python library for the same thing?  sure.\n",
        "# do i want to write a python utils file for the same thing?  not really but we may have to!\n",
        "\n",
        "#first pull the ticker list with google drive handler lib\n",
        "!git clone https://gist.github.com/dc7e60aa487430ea704a8cb3f2c5d6a6.git /tmp/colab_util_repo\n",
        "!mv /tmp/colab_util_repo/colab_util.py colab_util.py \n",
        "!rm -r /tmp/colab_util_repo\n",
        "from colab_util import *\n",
        "drive_handler = GoogleDriveHandler()\n",
        "\n",
        "drive_handler.download('test_ticker_list.csv', target_path='test_ticker_list.csv')\n",
        "#drive_handler.download('implementation_ticker_list.csv', target_path='implementation_ticker_list.csv')\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into '/tmp/colab_util_repo'...\n",
            "remote: Enumerating objects: 40, done.\u001b[K\n",
            "Unpacking objects:   2% (1/40)   \rUnpacking objects:   5% (2/40)   \rUnpacking objects:   7% (3/40)   \rUnpacking objects:  10% (4/40)   \rUnpacking objects:  12% (5/40)   \rUnpacking objects:  15% (6/40)   \rUnpacking objects:  17% (7/40)   \rUnpacking objects:  20% (8/40)   \rUnpacking objects:  22% (9/40)   \rUnpacking objects:  25% (10/40)   \rUnpacking objects:  27% (11/40)   \rUnpacking objects:  30% (12/40)   \rUnpacking objects:  32% (13/40)   \rUnpacking objects:  35% (14/40)   \rUnpacking objects:  37% (15/40)   \rUnpacking objects:  40% (16/40)   \rUnpacking objects:  42% (17/40)   \rremote: Total 40 (delta 0), reused 0 (delta 0), pack-reused 40\u001b[K\n",
            "Unpacking objects:  45% (18/40)   \rUnpacking objects:  47% (19/40)   \rUnpacking objects:  50% (20/40)   \rUnpacking objects:  52% (21/40)   \rUnpacking objects:  55% (22/40)   \rUnpacking objects:  57% (23/40)   \rUnpacking objects:  60% (24/40)   \rUnpacking objects:  62% (25/40)   \rUnpacking objects:  65% (26/40)   \rUnpacking objects:  67% (27/40)   \rUnpacking objects:  70% (28/40)   \rUnpacking objects:  72% (29/40)   \rUnpacking objects:  75% (30/40)   \rUnpacking objects:  77% (31/40)   \rUnpacking objects:  80% (32/40)   \rUnpacking objects:  82% (33/40)   \rUnpacking objects:  85% (34/40)   \rUnpacking objects:  87% (35/40)   \rUnpacking objects:  90% (36/40)   \rUnpacking objects:  92% (37/40)   \rUnpacking objects:  95% (38/40)   \rUnpacking objects:  97% (39/40)   \rUnpacking objects: 100% (40/40)   \rUnpacking objects: 100% (40/40), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04jdag6BhSmY",
        "colab_type": "code",
        "outputId": "c95ce1f8-8f31-4513-b37d-5db3ef0a1bca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "# activate R magic\n",
        "import rpy2\n",
        "%load_ext rpy2.ipython"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/rpy2/robjects/pandas2ri.py:14: FutureWarning: pandas.core.index is deprecated and will be removed in a future version.  The public classes are available in the top-level namespace.\n",
            "  from pandas.core.index import Index as PandasIndex\n",
            "/usr/local/lib/python3.6/dist-packages/rpy2/robjects/pandas2ri.py:34: UserWarning: pandas >= 1.0 is not supported.\n",
            "  warnings.warn('pandas >= 1.0 is not supported.')\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "The rpy2.ipython extension is already loaded. To reload it, use:\n",
            "  %reload_ext rpy2.ipython\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2x4-h6m0x-V",
        "colab_type": "code",
        "outputId": "55022d87-0dd9-4912-a72c-80a69050a709",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%%R\n",
        "\n",
        "#devtools::install_github(\"tidyverse/googlesheets4\")\n",
        "\n",
        "#devtools::install_version(\"xml2\", version = \"1.2.2\", repos = \"http://cran.us.r-project.org\")\n",
        "#file creates a set of csv from ticker list which include metadata & text data.\n",
        "\n",
        "devtools::install_github(\"mwaldstein/edgarWebR\")\n",
        "devtools::install_github(\"r-lib/xml2\") #this for edgarWebR \n",
        "#devtools::install_github('trinker/textclean')\n",
        "devtools::install_github(\"DavisVaughan/furrr\")\n",
        "\n",
        "#install.packages(\"pacman\") #would use this if we used CRAN libraries for the above\n",
        "\n",
        "library(edgarWebR) #this is an up to date library with an active maintainer.\n",
        "library(xml2)\n",
        "library(knitr)\n",
        "library(dplyr)\n",
        "library(purrr)\n",
        "library(rvest)\n",
        "library(tidyr)\n",
        "library(readr)\n",
        "#library(textclean)\n",
        "library(furrr)\n",
        "\n",
        "LOGFILE = format(Sys.time(), \"%b_%d_%Y.log\")\n",
        "print(LOGFILE)\n",
        "\n",
        "CSVFILE = format(Sys.time(), \"%b_%d_%Y.csv\")\n",
        "print(CSVFILE)\n",
        "\n",
        "get_filings_links <-function(str_ticker) {\n",
        "    df_filings <- company_filings(str_ticker, type = \"10-\", count = 20)\n",
        "    df_filings <- df_filings[df_filings$type == \"10-K\" | df_filings$type == \"10-Q\", ]\n",
        "    df_filing_infos <- map_df(df_filings$href, filing_information)\n",
        "    df_filings <- bind_cols(df_filings, df_filing_infos)\n",
        "    return(head(as_tibble(df_filings),20))\n",
        "  }\n",
        "\n",
        "write_log <- function(str_text) {\n",
        "      print(str_text)\n",
        "      if (file.exists(LOGFILE)) {\n",
        "          write(str_text,file=LOGFILE,append=TRUE)\n",
        "      } else {\n",
        "          write(str_text,file=LOGFILE,append=FALSE)\n",
        "      }\n",
        "\n",
        "  }\n",
        "\n",
        "write_log_csv <- function(df) {\n",
        "    if (file.exists(CSVFILE)) {\n",
        "          write_csv(df,CSVFILE,append=TRUE)\n",
        "      } else {\n",
        "          write_csv(df,CSVFILE,append=FALSE)\n",
        "      }\n",
        "\n",
        "  }\n",
        "\n",
        "get_mdna_text <- function(str_href) {\n",
        "  write_log(\"next link:\")\n",
        "  write_log(str_href)\n",
        "\n",
        "  #make this a func\n",
        "  str_file_path <- ''\n",
        "  file_path = strsplit(str_href,'/')\n",
        "  for (i in 5:length(file_path[[1]])-1) {\n",
        "    str_file_path = paste0(str_file_path,\"/\",(file_path[[1]][i]))\n",
        "  }\n",
        "  str_file_path <- paste0(getwd(),\"/\",str_file_path)\n",
        "  dir.create(str_file_path,recursive = TRUE)\n",
        "  str_file_path\n",
        "  str_file_name <- ''\n",
        "  file_path = strsplit(str_href,'/')\n",
        "  for (i in 4:length(file_path[[1]])) {\n",
        "    str_file_name = paste0(str_file_name,\"/\",(file_path[[1]][i]))\n",
        "  }\n",
        "  str_file_name <- paste0(getwd(),str_file_name)\n",
        "  str_file_name <- gsub(\".htm\",\".csv\",str_file_name)\n",
        "  \n",
        "  str_section = 'item 2|item 7'\n",
        "  str_search = 'discussion'\n",
        "\n",
        "  if (file.exists(str_file_name)) {  #add force equals true\n",
        "    write_log(\"filing documents from cache ...\")\n",
        "    \n",
        "    df_filing_documents <- read_csv(str_file_name,col_types = cols()) \n",
        "    df_filing_documents <- df_filing_documents %>% mutate_if(is.logical, as.character)\n",
        "  } else {\n",
        "    write_log(\"filing documents from sec ...\")\n",
        "    \n",
        "    df_filing_documents <- filing_documents(str_href) %>%\n",
        "      filter(!grepl('.pdf',href)) %>%\n",
        "      write_csv(str_file_name)\n",
        "  }\n",
        "  \n",
        "  str_doc_href <- df_filing_documents[df_filing_documents$type == \"10-K\" | df_filing_documents$type == \"10-Q\",]$href\n",
        "  \n",
        "  print(df_filing_documents[df_filing_documents$type == \"10-K\" | df_filing_documents$type == \"10-Q\",])  \n",
        "  \n",
        "  file_end <- gsub(\"https://www.sec.gov\",'',str_doc_href)\n",
        "  \n",
        "  file_name = paste0(getwd(),file_end)\n",
        "  \n",
        "  #use cache if possible\n",
        "  if (file.exists(file_name)) {\n",
        "\n",
        "    doc <- read_csv(file_name,col_types = cols(.default = \"c\"))\n",
        "    print(\"local cache\")\n",
        "    \n",
        "  } else {\n",
        "\n",
        "    doc <- parse_filing(str_doc_href)    \n",
        "\n",
        "    str_file_path <- ''\n",
        "    file_path = strsplit(file_name,'/')\n",
        "    for (i in 3:length(file_path[[1]])-1) {\n",
        "      str_file_path = paste0(str_file_path,\"/\",(file_path[[1]][i]))\n",
        "    }\n",
        "    str_file_path <- paste0(str_file_path,\"/\")\n",
        "    dir.create(str_file_path,recursive = TRUE)\n",
        "    write_csv(as_tibble(doc),file_name)\n",
        "    \n",
        "  }\n",
        "\n",
        "  df_txt <- doc[grepl(str_section, doc$item.name, ignore.case = TRUE) & grepl(str_search, doc$item.name, ignore.case = TRUE), ] # only discussion for now\n",
        "  #if default search fails, use a dictionary attempt\n",
        "  if (nrow(df_txt) == 0) {\n",
        "    write_log('going to backup')\n",
        "    #paired vector of start and ending text to slice if found\n",
        "    #going forward use tickers as an additional column\n",
        "    #and port this to a csv file as part of the install.\n",
        "    df_filter_list <- data.frame(\n",
        "      start_text = c('Introduction',\n",
        "                     'FUNCTIONAL EARNINGS', \n",
        "                     'DISCUSSION AND ANALYSIS',\n",
        "                     'DISCUSSION AND ANALYSIS',\n",
        "                     'DISCUSSION AND ANALYSIS',\n",
        "                     'OVERVIEW',\n",
        "                     'Business Overview',\n",
        "                     'Financial Review',\n",
        "                     'RESULTS OF OPERATIONS',\n",
        "                     'Overview',\n",
        "                     'Entergy operates',\n",
        "                     \"MANAGEMENT\\'S FINANCIAL DISCUSSION\",\n",
        "                     'General',\n",
        "                     \"Management's Discussion\",\n",
        "                     'EXECUTIVE SUMMARY',\n",
        "                     'EXECUTIVE OVERVIEW',\n",
        "                     'EXECUTIVE OVERVIEW',\n",
        "                     'The following management discussion and analysis',\n",
        "                     'CURRENT ECONOMIC CONDITIONS',\n",
        "                     'Overview and Highlights',\n",
        "                     'Financial Review - Results of Operations'),\n",
        "      end_text = c('Quantitative and qualitative disclosures about market risk',\n",
        "                   \"MANAGEMENT\\'S REPORT\",\n",
        "                   'RISK FACTORS',\n",
        "                   'FIVE-YEAR PERFORMANCE GRAPH',\n",
        "                   'FINANCIAL STATEMENTS AND NOTES',\n",
        "                   'Risk management includes the identification',\n",
        "                   'Selected Loan Maturity Data',\n",
        "                   'Risk Management',\n",
        "                   'QUANTITATIVE AND QUALITATIVE DISCLOSURES',\n",
        "                   'Forward-Looking Statements',\n",
        "                   'New Accounting Pronouncements',\n",
        "                   'New Accounting Pronouncements',\n",
        "                   'Website information',\n",
        "                   'Risk Disclosures',\n",
        "                   'RISK FACTORS',\n",
        "                   'A summary of contractual obligations is included',\n",
        "                   'CONSOLIDATED RESULTS OF OPERATIONS',\n",
        "                   'NON-GAAP FINANCIAL MEASURES',\n",
        "                   'FORWARD-LOOKING STATEMENTS',\n",
        "                   'Critical Accounting Policies and Estimates',\n",
        "                   'Unregistered Sales of Equity Securities and Use of Proceeds')\n",
        "    )\n",
        "    \n",
        "    #this would be case sensitive\n",
        "    for (row in 1:nrow(df_filter_list)) { #should flip this to apply()\n",
        "\n",
        "      start_text <- df_filter_list[row, \"start_text\"]\n",
        "      end_text <- df_filter_list[row, \"end_text\"]\n",
        "\n",
        "      write_log(paste0('trying ',start_text))\n",
        "      write_log(paste0('to ',end_text))\n",
        "\n",
        "      i_start = as.integer(which(grepl(start_text, doc$text))) \n",
        "      if (length(i_start) > 1) { #handle table of contents duplicates\n",
        "        i_start = i_start[2]\n",
        "      }\n",
        "      i_end = as.integer(which(grepl(end_text, doc$text)))\n",
        "      if (length(i_end) > 1) {\n",
        "        i_end = i_end[2]\n",
        "      }\n",
        "\n",
        "      write_log(i_start)\n",
        "      write_log(i_end)\n",
        "\n",
        "      if (length(i_start) != 0 & length(i_end) != 0) {\n",
        "        #i_start = as.numeric(i_start)\n",
        "        #i_end = as.numeric(i_end)\n",
        "        if (i_start < i_end) {        \n",
        "            print(paste0('istart is:',i_start,' iend is:',i_end))\n",
        "            df_txt = doc[i_start:i_end,]\n",
        "            break\n",
        "        }\n",
        "      }\n",
        "\n",
        "    }\n",
        "    if (length(i_start) == 0 || length(i_end) == 0) {\n",
        "      write_log(\"missing section for:\")\n",
        "      write_log(str_href)\n",
        "    }\n",
        "\n",
        "  }\n",
        "  #we could do some text preprocessing here.\n",
        "\n",
        "  df_txt <- as_tibble(df_txt) %>%\n",
        "    #mutate(text = textclean::strip(text)) %>%\n",
        "    mutate(section = str_search)\n",
        "\n",
        "  return(df_txt)\n",
        "}\n",
        "\n",
        "get_document_text <- function(str_ticker, force = FALSE) { #not using force yet\n",
        "  start_time <- Sys.time()\n",
        "\n",
        "  write_log(str_ticker)\n",
        "\n",
        "  str_write_name <- paste0('sec_data_folder/',str_ticker)\n",
        "\n",
        "  write_log(\"get filings links ...\")\n",
        "\n",
        "  filings_csv <- paste0(str_write_name,\"_filings.csv\")\n",
        "  \n",
        "  if (file.exists(filings_csv)) {  #add force equals true\n",
        "    write_log(\"from cache ...\")\n",
        "    \n",
        "    df_filings <- read_csv(filings_csv,col_types = cols()) \n",
        "    df_filings <- df_filings %>% mutate_if(is.logical, as.character)\n",
        "    } else {\n",
        "    write_log(\"from sec ...\")\n",
        "      \n",
        "    df_filings <- get_filings_links(str_ticker) %>%\n",
        "      mutate(ticker = str_ticker) %>%\n",
        "      write_csv(filings_csv)\n",
        "    }\n",
        "\n",
        "  write_log_csv(df_filings)\n",
        "  \n",
        "#for debug\n",
        "  i_test = nrow(df_filings) #for some reason this won't evaulate inside the if statement\n",
        "  if (i_test == 0) {\n",
        "      return(NULL)\n",
        "  }\n",
        "\n",
        "  write_log(\"get section text ...\")\n",
        "\n",
        "  df_data <- (df_filings) %>%\n",
        "    rowwise() %>%\n",
        "    mutate(nest_discussion = map(.x = href, .f = get_mdna_text)) %>%\n",
        "    ungroup() %>%\n",
        "    group_by(period_date) %>%\n",
        "    arrange(desc(period_date))\n",
        "\n",
        "  #jenky - find a rowwise application\n",
        "  a <- df_data %>%\n",
        "    select(period_date,filing_date,type,form_name,documents,nest_discussion) %>%\n",
        "    unnest(nest_discussion)\n",
        "\n",
        "  write_log(\"write to local csv  ...\")\n",
        "  df_data <- a %>%\n",
        "    as_tibble() %>%\n",
        "    write_csv(paste0(str_write_name,\".csv\"))\n",
        "\n",
        "  end_time <- Sys.time()\n",
        "\n",
        "  write_log(end_time - start_time)\n",
        "\n",
        "  return(df_data)\n",
        "}\n",
        "\n",
        "df_tickers <- read_csv('test_ticker_list.csv',col_types = cols()) \n",
        "\n",
        "dir.create('sec_data_folder', showWarnings = FALSE)\n",
        "\n",
        "future::plan(multiprocess)\n",
        "\n",
        "df_data <- future_map_dfr(df_tickers$Symbol, get_document_text,.progress = TRUE)\n",
        "#df_data <- map_df(df_tickers$Symbol, get_document_text)\n",
        "print('done')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "R[write to console]: Downloading GitHub repo mwaldstein/edgarWebR@master\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "curl (4.2 -> 4.3) [CRAN]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "R[write to console]: Installing 1 packages: curl\n",
            "\n",
            "R[write to console]: Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n",
            "R[write to console]: trying URL 'https://cran.rstudio.com/src/contrib/curl_4.3.tar.gz'\n",
            "\n",
            "R[write to console]: Content type 'application/x-gzip'\n",
            "R[write to console]:  length 673779 bytes (657 KB)\n",
            "\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: \n",
            "\n",
            "R[write to console]: downloaded 657 KB\n",
            "\n",
            "\n",
            "R[write to console]: \n",
            "\n",
            "R[write to console]: \n",
            "R[write to console]: The downloaded source packages are in\n",
            "\t‘/tmp/RtmpCN13mP/downloaded_packages’\n",
            "R[write to console]: \n",
            "R[write to console]: \n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "✔  checking for file ‘/tmp/RtmpCN13mP/remotes8b177475e2/mwaldstein-edgarWebR-e7fa70e/DESCRIPTION’\n",
            "─  preparing ‘edgarWebR’:\n",
            "✔  checking DESCRIPTION meta-information\n",
            "─  checking for LF line-endings in source and make files and shell scripts\n",
            "─  checking for empty or unneeded directories\n",
            "─  looking to see if a ‘data/datalist’ file should be added\n",
            "─  building ‘edgarWebR_1.0.2.tar.gz’\n",
            "   \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "R[write to console]: Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n",
            "R[write to console]: Downloading GitHub repo r-lib/xml2@master\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "✔  checking for file ‘/tmp/RtmpCN13mP/remotes8b6cb4a8e1/r-lib-xml2-876759f/DESCRIPTION’\n",
            "─  preparing ‘xml2’:\n",
            "✔  checking DESCRIPTION meta-information\n",
            "─  cleaning src\n",
            "─  running ‘cleanup’\n",
            "─  installing the package to process help pages\n",
            "─  cleaning src (8.1s)\n",
            "─  running ‘cleanup’\n",
            "─  checking for LF line-endings in source and make files and shell scripts\n",
            "─  checking for empty or unneeded directories\n",
            "─  building ‘xml2_1.3.2.9000.tar.gz’\n",
            "   \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "R[write to console]: Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1UYcX52F7_pu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#switch from R to python\n",
        "!pip install pandarallel\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('vader_lexicon')\n",
        "nltk.download('stopwords')\n",
        "#from transformers import pipeline\n",
        "#nlp = pipeline('sentiment-analysis')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from nltk import tokenize\n",
        "import pandas as pd\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import os.path\n",
        "from os import path\n",
        "\n",
        "import numpy as np\n",
        "from pandarallel import pandarallel\n",
        "\n",
        "import time\n",
        "\n",
        "pandarallel.initialize()\n",
        "df_tickers = pd.read_csv('test_ticker_list.csv')\n",
        "\n",
        "LOGFILE = 'sec_nlp_beta.log'\n",
        "f = open(LOGFILE, \"w\")\n",
        "t = time.localtime()\n",
        "current_time = time.strftime(\"%H:%M:%S\", t)\n",
        "f.write(current_time+\": process started\")\n",
        "f.close()\n",
        "\n",
        "FIND_WORDS = ['covid',\n",
        "              'guidance',\n",
        "              'outlook']\n",
        "\n",
        "def check_if_list_found_in_text(text, words=[], return_offset=False, lower_text=True):\n",
        "    result = []\n",
        "    text = (\n",
        "        \" \"\n",
        "        + text.replace(\"_\", \" \")\n",
        "        .replace(\"-\", \" \")\n",
        "        .replace(\",\", \" \")\n",
        "        .replace(\";\", \" \")\n",
        "        .replace('\"', \" \")\n",
        "        .replace(\":\", \" \")\n",
        "        .replace(\".\", \" \")\n",
        "        + \" \"\n",
        "    )\n",
        "    if lower_text:\n",
        "        text = text.lower()\n",
        "    for word in words:\n",
        "        word = (\n",
        "            \" \"\n",
        "            + word.replace(\"_\", \" \")\n",
        "            .replace(\"-\", \" \")\n",
        "            .replace(\",\", \" \")\n",
        "            .replace(\";\", \" \")\n",
        "            .replace('\"', \" \")\n",
        "            .replace(\":\", \" \")\n",
        "            .replace(\".\", \" \")\n",
        "            + \" \"\n",
        "        )\n",
        "        if lower_text:\n",
        "            word = word.lower()\n",
        "        if word in text:\n",
        "            if return_offset:\n",
        "                offset = text.find(word)\n",
        "                # offset = offset if not offset else offset-1\n",
        "                result.append(offset)\n",
        "            else:\n",
        "                result.append(word.strip())\n",
        "    return result\n",
        "\n",
        "def filter_stopwords(sent):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    word_tokens = word_tokenize(sent)\n",
        "    filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
        "    filtered_sentence = []\n",
        "    for w in word_tokens:\n",
        "        if w not in stop_words:\n",
        "            filtered_sentence.append(w)\n",
        "    return ' '.join(filtered_sentence)\n",
        "\n",
        "def sentiment_from_text(sentence):\n",
        "#  dict_sentiment = {}\n",
        "  sentence = filter_stopwords(sentence)\n",
        "  list_found = check_if_list_found_in_text(sentence,FIND_WORDS)\n",
        "  num_found = len(list_found)\n",
        "  ###\n",
        "  ss = sid.polarity_scores(sentence) #NLTK\n",
        "  df = pd.DataFrame.from_dict(ss,orient = \"index\").T\n",
        "  df['transformers_score'] = dict_transformers['score'] #tranformers\n",
        "  df['transformers_label'] = dict_transformers['label']\n",
        "  df['text'] = sentence\n",
        "  df['keywords_found'] = num_found\n",
        "  return pd.concat(dict_sentiment)\n",
        "\n",
        "  ###\n",
        "\n",
        "def filter_stopwords(sent):\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  word_tokens = word_tokenize(sent)\n",
        "  filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
        "  filtered_sentence = []\n",
        "  for w in word_tokens:\n",
        "      if w not in stop_words:\n",
        "          filtered_sentence.append(w)\n",
        "  return ' '.join(filtered_sentence)\n",
        "\n",
        "def df_from_text(text):\n",
        "  sentence_list = tokenize.sent_tokenize(text)\n",
        "  sentence_list\n",
        "  sid = SentimentIntensityAnalyzer()\n",
        "  list_df = []\n",
        "  for sentence in sentence_list:\n",
        "      sentence = filter_stopwords(sentence)\n",
        "      list_found = check_if_list_found_in_text(sentence,FIND_WORDS)\n",
        "      num_found = len(list_found)\n",
        "# Allocate a pipeline for sentiment-analysis\n",
        "#nlp = pipeline('sentiment-analysis')\n",
        "#nlp('We are very happy to include pipeline into the transformers repository.')\n",
        "#>>> {'label': 'POSITIVE', 'score': 0.99893874}\n",
        "## which contains pos, neg, neu, and compound scores.\n",
        "\n",
        "      ss = sid.polarity_scores(sentence)\n",
        "      df = pd.DataFrame.from_dict(ss,orient = \"index\").T\n",
        "      df['text'] = sentence\n",
        "      df['keywords_found'] = num_found\n",
        "      list_df.append(df)\n",
        "      return pd.concat(list_df)\n",
        "\n",
        "def py_write_log(str_text):\n",
        "    t = time.localtime()\n",
        "    current_time = time.strftime(\"%H:%M:%S\", t)\n",
        "    print(str_text)\n",
        "    f = open(LOGFILE, \"a\")\n",
        "    f.write(current_time+\": \"+str_text)\n",
        "    f.close()\n",
        "    return\n",
        "\n",
        "def func_sentiment(row):\n",
        "    df = df_from_text(row['text']) #neg neu pos compound text keywords_found\n",
        "    neu = df.iloc[0]['neu']\n",
        "    pos = df.iloc[0]['pos']\n",
        "    neg = df.iloc[0]['neg']\n",
        "    num_rows = 1\n",
        "    compound = df.iloc[0]['compound']\n",
        "    text = df.iloc[0]['text']\n",
        "    keywords_found = df.iloc[0]['keywords_found']\n",
        "    return pd.Series([row['ticker'],row['section'],row['type'],row['period_date'],neu,pos,neg,compound,keywords_found,text,num_rows])\n",
        "\n",
        "master_list_df = []\n",
        "list_tickers = df_tickers['Symbol']\n",
        "#list_tickers = ['MMM']\n",
        "\n",
        "for ticker in list_tickers:\n",
        "    py_write_log(\"working on...\"+ticker)\n",
        "    tic = time.perf_counter()\n",
        "\n",
        "    if path.exists(\"sec_data_folder/\"+ticker+\".csv\"):\n",
        "\n",
        "        df_text = pd.read_csv(\"sec_data_folder/\"+ticker+\".csv\")\n",
        "        if len(df_text) > 0:\n",
        "\n",
        "            df_text['ticker'] = ticker\n",
        "\n",
        "            df_discussion = df_text[df_text['section']=='discussion']\n",
        "\n",
        "            df_out = df_discussion.parallel_apply(func_sentiment, axis=1)\n",
        "            df_out.columns = ['ticker','section','type','period_date','neu','neg','pos','compound','keywords_found','text','num_rows']\n",
        "            #df_out.to_csv(\"test.csv\")\n",
        "            if len(df_out) > 0:\n",
        "\n",
        "                df_out = df_out.groupby(['ticker','period_date','type']).sum().reset_index()\n",
        "                df_out['neg'] = df_out['neg']/df_out['num_rows']\n",
        "                df_out['neu'] = df_out['neu']/df_out['num_rows']\n",
        "                df_out['pos'] = df_out['pos']/df_out['num_rows']\n",
        "                df_out['compound'] = df_out['compound']/df_out['num_rows']\n",
        "\n",
        "                df_error = df_out[df_out['compound'] == 0]\n",
        "                if not df_error.empty:\n",
        "                    py_write_log(\"zero values...\"+ticker)\n",
        "                    df_error.to_csv('sec_nlp_errors.csv',mode = 'a')\n",
        "\n",
        "                df_out.drop(['keywords_found'],axis = 1)\n",
        "                df_out['compound_baseline'] = df_out['compound'] / df_out['compound'].mean()\n",
        "                df_out['neg_baseline'] = df_out['neg'] / df_out['neg'].mean()\n",
        "                df_out['pos_baseline'] = df_out['pos'] / df_out['pos'].mean()\n",
        "                df_out['compound_bdiff'] = df_out['compound_baseline'].diff()\n",
        "                df_out['neg_bdiff'] = df_out['neg_baseline'].diff()\n",
        "                df_out['pos_bdiff'] = df_out['pos_baseline'].diff()\n",
        "                df_out['compound_zscore'] = (df_out['compound'] - df_out['compound'].mean())/df_out['compound'].std(ddof=0)\n",
        "\n",
        "                #always cache\n",
        "                str_score_file = \"sec_data_folder/\"+ticker+\"_score.csv\"\n",
        "                df_out.to_csv(str_score_file)\n",
        "\n",
        "                master_list_df.append(df_out)\n",
        "            else:\n",
        "                py_write_log(\"missing...\"+ticker)\n",
        "        else:\n",
        "            py_write_log(ticker+\" has no data.\")\n",
        "    toc = time.perf_counter()\n",
        "    py_write_log(f\"Text processed in {toc - tic:0.4f} seconds\")\n",
        "\n",
        "if master_list_df:\n",
        "    df_data = pd.concat(master_list_df)\n",
        "    df_data.to_csv('df_data.csv')\n",
        "\n",
        "df = df_data[['period_date','ticker','compound_baseline']]\n",
        "df['quarter_end'] = pd.to_datetime(df['period_date'])\n",
        "df['quarter_end'] = df.quarter_end.map(lambda x: x.strftime('%Y-%m-%d'))\n",
        "\n",
        "#modify odd quarter ends\n",
        "df.loc[df.quarter_end == '2017-04-01', 'quarter_end'] = '2017-03-31'\n",
        "df.loc[df.quarter_end == '2017-07-01', 'quarter_end'] = '2017-06-30'\n",
        "df.loc[df.quarter_end == '2018-04-01', 'quarter_end'] = '2018-03-31'\n",
        "df.loc[df.quarter_end == '2018-07-01', 'quarter_end'] = '2018-06-30'\n",
        "\n",
        "df['quarter_end'] = pd.to_datetime(df['quarter_end'])\n",
        "df['quarter_end'] = df['quarter_end'].dt.to_period('q').dt.end_time #floor at end of quarter\n",
        "df['quarter_end'] = df.quarter_end.map(lambda x: x.strftime('%Y-%m-%d')) #format\n",
        "\n",
        "import numpy as np\n",
        "df_data_pivot = pd.pivot_table(df, values='compound_baseline', index=['quarter_end'],\n",
        "                columns=['ticker'], aggfunc=np.sum, fill_value=0).reset_index()\n",
        "\n",
        "df_data_pivot.to_csv(\"df_data_pivot.csv\")\n",
        "\n",
        "print('done!')\n",
        "\n",
        "#example plot\n",
        "#df_plot = df_data[df_data['ticker'] == 'XOM']\n",
        "\n",
        "#df_plot['period_date'] = pd.to_datetime(df_plot['period_date'])\n",
        "#df_plot['period_date'] = df_plot.period_date.map(lambda x: x.strftime('%Y-%m-%d')) #format\n",
        "\n",
        "#df_plot.compound = pd.to_numeric(df_plot.compound)\n",
        "\n",
        "#df_plot.plot.bar(x='period_date', y='compound', rot=90,title='XOM')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iuyztVzxz7uo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bg7crMy1XhAd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_plot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4BkP1_E2F0H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2hhgvMQhfog7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_plot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Z0zkYVtf-su",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S55HBJIcf_mD",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sec_test.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/reckoning-machines/sec_test/blob/master/sec_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MxREunso0REA",
        "colab_type": "code",
        "outputId": "a8047749-fa85-4edd-d6b8-36d67db9069c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "# why use R here?  \n",
        "# edgarWebR pulls sections really well\n",
        "!rm -r 'sec_test'\n",
        "!git clone https://github.com/reckoning-machines/sec_test.git #errors if exists but doesn't halt exe\n",
        "!cp \"sec_test/test_ticker_list.csv\" \"test_ticker_list.csv\" #move ticker list to root\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'sec_test'...\n",
            "remote: Enumerating objects: 28, done.\u001b[K\n",
            "remote: Counting objects:   3% (1/28)\u001b[K\rremote: Counting objects:   7% (2/28)\u001b[K\rremote: Counting objects:  10% (3/28)\u001b[K\rremote: Counting objects:  14% (4/28)\u001b[K\rremote: Counting objects:  17% (5/28)\u001b[K\rremote: Counting objects:  21% (6/28)\u001b[K\rremote: Counting objects:  25% (7/28)\u001b[K\rremote: Counting objects:  28% (8/28)\u001b[K\rremote: Counting objects:  32% (9/28)\u001b[K\rremote: Counting objects:  35% (10/28)\u001b[K\rremote: Counting objects:  39% (11/28)\u001b[K\rremote: Counting objects:  42% (12/28)\u001b[K\rremote: Counting objects:  46% (13/28)\u001b[K\rremote: Counting objects:  50% (14/28)\u001b[K\rremote: Counting objects:  53% (15/28)\u001b[K\rremote: Counting objects:  57% (16/28)\u001b[K\rremote: Counting objects:  60% (17/28)\u001b[K\rremote: Counting objects:  64% (18/28)\u001b[K\rremote: Counting objects:  67% (19/28)\u001b[K\rremote: Counting objects:  71% (20/28)\u001b[K\rremote: Counting objects:  75% (21/28)\u001b[K\rremote: Counting objects:  78% (22/28)\u001b[K\rremote: Counting objects:  82% (23/28)\u001b[K\rremote: Counting objects:  85% (24/28)\u001b[K\rremote: Counting objects:  89% (25/28)\u001b[K\rremote: Counting objects:  92% (26/28)\u001b[K\rremote: Counting objects:  96% (27/28)\u001b[K\rremote: Counting objects: 100% (28/28)\u001b[K\rremote: Counting objects: 100% (28/28), done.\u001b[K\n",
            "remote: Compressing objects:   3% (1/27)\u001b[K\rremote: Compressing objects:   7% (2/27)\u001b[K\rremote: Compressing objects:  11% (3/27)\u001b[K\rremote: Compressing objects:  14% (4/27)\u001b[K\rremote: Compressing objects:  18% (5/27)\u001b[K\rremote: Compressing objects:  22% (6/27)\u001b[K\rremote: Compressing objects:  25% (7/27)\u001b[K\rremote: Compressing objects:  29% (8/27)\u001b[K\rremote: Compressing objects:  33% (9/27)\u001b[K\rremote: Compressing objects:  37% (10/27)\u001b[K\rremote: Compressing objects:  40% (11/27)\u001b[K\rremote: Compressing objects:  44% (12/27)\u001b[K\rremote: Compressing objects:  48% (13/27)\u001b[K\rremote: Compressing objects:  51% (14/27)\u001b[K\rremote: Compressing objects:  55% (15/27)\u001b[K\rremote: Compressing objects:  59% (16/27)\u001b[K\rremote: Compressing objects:  62% (17/27)\u001b[K\rremote: Compressing objects:  66% (18/27)\u001b[K\rremote: Compressing objects:  70% (19/27)\u001b[K\rremote: Compressing objects:  74% (20/27)\u001b[K\rremote: Compressing objects:  77% (21/27)\u001b[K\rremote: Compressing objects:  81% (22/27)\u001b[K\rremote: Compressing objects:  85% (23/27)\u001b[K\rremote: Compressing objects:  88% (24/27)\u001b[K\rremote: Compressing objects:  92% (25/27)\u001b[K\rremote: Compressing objects:  96% (26/27)\u001b[K\rremote: Compressing objects: 100% (27/27)\u001b[K\rremote: Compressing objects: 100% (27/27), done.\u001b[K\n",
            "Unpacking objects:   3% (1/28)   \rUnpacking objects:   7% (2/28)   \rUnpacking objects:  10% (3/28)   \rUnpacking objects:  14% (4/28)   \rUnpacking objects:  17% (5/28)   \rUnpacking objects:  21% (6/28)   \rUnpacking objects:  25% (7/28)   \rUnpacking objects:  28% (8/28)   \rUnpacking objects:  32% (9/28)   \rUnpacking objects:  35% (10/28)   \rUnpacking objects:  39% (11/28)   \rUnpacking objects:  42% (12/28)   \rremote: Total 28 (delta 15), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects:  46% (13/28)   \rUnpacking objects:  50% (14/28)   \rUnpacking objects:  53% (15/28)   \rUnpacking objects:  57% (16/28)   \rUnpacking objects:  60% (17/28)   \rUnpacking objects:  64% (18/28)   \rUnpacking objects:  67% (19/28)   \rUnpacking objects:  71% (20/28)   \rUnpacking objects:  75% (21/28)   \rUnpacking objects:  78% (22/28)   \rUnpacking objects:  82% (23/28)   \rUnpacking objects:  85% (24/28)   \rUnpacking objects:  89% (25/28)   \rUnpacking objects:  92% (26/28)   \rUnpacking objects:  96% (27/28)   \rUnpacking objects: 100% (28/28)   \rUnpacking objects: 100% (28/28), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04jdag6BhSmY",
        "colab_type": "code",
        "outputId": "bd61ecab-786b-44fa-e2d3-f07d994cabda",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "# activate R magic\n",
        "import rpy2\n",
        "%load_ext rpy2.ipython"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/rpy2/robjects/pandas2ri.py:14: FutureWarning: pandas.core.index is deprecated and will be removed in a future version.  The public classes are available in the top-level namespace.\n",
            "  from pandas.core.index import Index as PandasIndex\n",
            "/usr/local/lib/python3.6/dist-packages/rpy2/robjects/pandas2ri.py:34: UserWarning: pandas >= 1.0 is not supported.\n",
            "  warnings.warn('pandas >= 1.0 is not supported.')\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OSYJWc0DyRFR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4346e605-56b5-4b41-b6c7-851a9b756f6c"
      },
      "source": [
        "%%R\n",
        "\n",
        "#installs take a bit of time...\n",
        "devtools::install_github(\"mwaldstein/edgarWebR\")\n",
        "devtools::install_github(\"r-lib/xml2\") #this for edgarWebR \n",
        "#devtools::install_github(\"DavisVaughan/furrr\")\n",
        "install.packages('furrr')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "R[write to console]: Skipping install of 'edgarWebR' from a github remote, the SHA1 (e7fa70ea) has not changed since last install.\n",
            "  Use `force = TRUE` to force installation\n",
            "\n",
            "R[write to console]: Skipping install of 'xml2' from a github remote, the SHA1 (876759f3) has not changed since last install.\n",
            "  Use `force = TRUE` to force installation\n",
            "\n",
            "R[write to console]: Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n",
            "R[write to console]: trying URL 'https://cran.rstudio.com/src/contrib/furrr_0.1.0.tar.gz'\n",
            "\n",
            "R[write to console]: Content type 'application/x-gzip'\n",
            "R[write to console]:  length 75030 bytes (73 KB)\n",
            "\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: =\n",
            "R[write to console]: \n",
            "\n",
            "R[write to console]: downloaded 73 KB\n",
            "\n",
            "\n",
            "R[write to console]: \n",
            "\n",
            "R[write to console]: \n",
            "R[write to console]: The downloaded source packages are in\n",
            "\t‘/tmp/RtmpidVQMG/downloaded_packages’\n",
            "R[write to console]: \n",
            "R[write to console]: \n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2x4-h6m0x-V",
        "colab_type": "code",
        "outputId": "c22eb157-2dec-4fb1-d14e-7510947c39f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        }
      },
      "source": [
        "%%R\n",
        "\n",
        "\n",
        "library(edgarWebR) #this is an up to date library with an active maintainer.\n",
        "library(xml2)\n",
        "library(knitr)\n",
        "library(dplyr)\n",
        "library(purrr)\n",
        "library(rvest)\n",
        "library(tidyr)\n",
        "library(readr)\n",
        "#library(textclean) #not using this yet.\n",
        "library(furrr) #multiprocessing... does colab use it\n",
        "\n",
        "LOGFILE = format(Sys.time(), \"%b_%d_%Y.log\")\n",
        "print(LOGFILE)\n",
        "\n",
        "CSVFILE = format(Sys.time(), \"%b_%d_%Y.csv\")\n",
        "print(CSVFILE)\n",
        "\n",
        "get_filings_links <-function(str_ticker) {\n",
        "    df_filings <- company_filings(str_ticker, type = \"10-\", count = 20)\n",
        "    df_filings <- df_filings[df_filings$type == \"10-K\" | df_filings$type == \"10-Q\", ]\n",
        "    df_filing_infos <- map_df(df_filings$href, filing_information)\n",
        "    df_filings <- bind_cols(df_filings, df_filing_infos)\n",
        "    return(head(as_tibble(df_filings),20))\n",
        "  }\n",
        "\n",
        "write_log <- function(str_text) {\n",
        "      print(str_text)\n",
        "      if (file.exists(LOGFILE)) {\n",
        "          write(str_text,file=LOGFILE,append=TRUE)\n",
        "      } else {\n",
        "          write(str_text,file=LOGFILE,append=FALSE)\n",
        "      }\n",
        "\n",
        "  }\n",
        "\n",
        "write_log_csv <- function(df) {\n",
        "    if (file.exists(CSVFILE)) {\n",
        "          write_csv(df,CSVFILE,append=TRUE)\n",
        "      } else {\n",
        "          write_csv(df,CSVFILE,append=FALSE)\n",
        "      }\n",
        "\n",
        "  }\n",
        "\n",
        "get_mdna_text <- function(str_href) {\n",
        "  write_log(\"next link:\")\n",
        "  write_log(str_href)\n",
        "\n",
        "  #make this a func\n",
        "  str_file_path <- ''\n",
        "  file_path = strsplit(str_href,'/')\n",
        "  for (i in 5:length(file_path[[1]])-1) {\n",
        "    str_file_path = paste0(str_file_path,\"/\",(file_path[[1]][i]))\n",
        "  }\n",
        "  str_file_path <- paste0(getwd(),\"/\",str_file_path)\n",
        "  dir.create(str_file_path,recursive = TRUE)\n",
        "  str_file_path\n",
        "  str_file_name <- ''\n",
        "  file_path = strsplit(str_href,'/')\n",
        "  for (i in 4:length(file_path[[1]])) {\n",
        "    str_file_name = paste0(str_file_name,\"/\",(file_path[[1]][i]))\n",
        "  }\n",
        "  str_file_name <- paste0(getwd(),str_file_name)\n",
        "  str_file_name <- gsub(\".htm\",\".csv\",str_file_name)\n",
        "  \n",
        "  str_section = 'item 2|item 7'\n",
        "  str_search = 'discussion'\n",
        "\n",
        "  if (file.exists(str_file_name)) {  #add force equals true\n",
        "    write_log(\"filing documents from cache ...\")\n",
        "    \n",
        "    df_filing_documents <- read_csv(str_file_name,col_types = cols()) \n",
        "    df_filing_documents <- df_filing_documents %>% mutate_if(is.logical, as.character)\n",
        "  } else {\n",
        "    write_log(\"filing documents from sec ...\")\n",
        "    \n",
        "    df_filing_documents <- filing_documents(str_href) %>%\n",
        "      filter(!grepl('.pdf',href)) %>%\n",
        "      write_csv(str_file_name)\n",
        "  }\n",
        "  \n",
        "  str_doc_href <- df_filing_documents[df_filing_documents$type == \"10-K\" | df_filing_documents$type == \"10-Q\",]$href\n",
        "  \n",
        "  print(df_filing_documents[df_filing_documents$type == \"10-K\" | df_filing_documents$type == \"10-Q\",])  \n",
        "  \n",
        "  file_end <- gsub(\"https://www.sec.gov\",'',str_doc_href)\n",
        "  \n",
        "  file_name = paste0(getwd(),file_end)\n",
        "  \n",
        "  #use cache if possible\n",
        "  if (file.exists(file_name)) {\n",
        "\n",
        "    doc <- read_csv(file_name,col_types = cols(.default = \"c\"))\n",
        "    print(\"local cache\")\n",
        "    \n",
        "  } else {\n",
        "\n",
        "    doc <- parse_filing(str_doc_href)    \n",
        "\n",
        "    str_file_path <- ''\n",
        "    file_path = strsplit(file_name,'/')\n",
        "    for (i in 3:length(file_path[[1]])-1) {\n",
        "      str_file_path = paste0(str_file_path,\"/\",(file_path[[1]][i]))\n",
        "    }\n",
        "    str_file_path <- paste0(str_file_path,\"/\")\n",
        "    dir.create(str_file_path,recursive = TRUE)\n",
        "    write_csv(as_tibble(doc),file_name)\n",
        "    \n",
        "  }\n",
        "\n",
        "  df_txt <- doc[grepl(str_section, doc$item.name, ignore.case = TRUE) & grepl(str_search, doc$item.name, ignore.case = TRUE), ] # only discussion for now\n",
        "  #if default search fails, use a dictionary attempt\n",
        "  if (nrow(df_txt) == 0) {\n",
        "    write_log('going to backup')\n",
        "    #paired vector of start and ending text to slice if found\n",
        "    #going forward use tickers as an additional column\n",
        "    #and port this to a csv file as part of the install.\n",
        "    df_filter_list <- data.frame(\n",
        "      start_text = c('Introduction',\n",
        "                     'FUNCTIONAL EARNINGS', \n",
        "                     'DISCUSSION AND ANALYSIS',\n",
        "                     'DISCUSSION AND ANALYSIS',\n",
        "                     'DISCUSSION AND ANALYSIS',\n",
        "                     'OVERVIEW',\n",
        "                     'Business Overview',\n",
        "                     'Financial Review',\n",
        "                     'RESULTS OF OPERATIONS',\n",
        "                     'Overview',\n",
        "                     'Entergy operates',\n",
        "                     \"MANAGEMENT\\'S FINANCIAL DISCUSSION\",\n",
        "                     'General',\n",
        "                     \"Management's Discussion\",\n",
        "                     'EXECUTIVE SUMMARY',\n",
        "                     'EXECUTIVE OVERVIEW',\n",
        "                     'EXECUTIVE OVERVIEW',\n",
        "                     'The following management discussion and analysis',\n",
        "                     'CURRENT ECONOMIC CONDITIONS',\n",
        "                     'Overview and Highlights',\n",
        "                     'Financial Review - Results of Operations'),\n",
        "      end_text = c('Quantitative and qualitative disclosures about market risk',\n",
        "                   \"MANAGEMENT\\'S REPORT\",\n",
        "                   'RISK FACTORS',\n",
        "                   'FIVE-YEAR PERFORMANCE GRAPH',\n",
        "                   'FINANCIAL STATEMENTS AND NOTES',\n",
        "                   'Risk management includes the identification',\n",
        "                   'Selected Loan Maturity Data',\n",
        "                   'Risk Management',\n",
        "                   'QUANTITATIVE AND QUALITATIVE DISCLOSURES',\n",
        "                   'Forward-Looking Statements',\n",
        "                   'New Accounting Pronouncements',\n",
        "                   'New Accounting Pronouncements',\n",
        "                   'Website information',\n",
        "                   'Risk Disclosures',\n",
        "                   'RISK FACTORS',\n",
        "                   'A summary of contractual obligations is included',\n",
        "                   'CONSOLIDATED RESULTS OF OPERATIONS',\n",
        "                   'NON-GAAP FINANCIAL MEASURES',\n",
        "                   'FORWARD-LOOKING STATEMENTS',\n",
        "                   'Critical Accounting Policies and Estimates',\n",
        "                   'Unregistered Sales of Equity Securities and Use of Proceeds')\n",
        "    )\n",
        "    \n",
        "    #this would be case sensitive\n",
        "    for (row in 1:nrow(df_filter_list)) { #should flip this to apply()\n",
        "\n",
        "      start_text <- df_filter_list[row, \"start_text\"]\n",
        "      end_text <- df_filter_list[row, \"end_text\"]\n",
        "\n",
        "      write_log(paste0('trying ',start_text))\n",
        "      write_log(paste0('to ',end_text))\n",
        "\n",
        "      i_start = as.integer(which(grepl(start_text, doc$text))) \n",
        "      if (length(i_start) > 1) { #handle table of contents duplicates\n",
        "        i_start = i_start[2]\n",
        "      }\n",
        "      i_end = as.integer(which(grepl(end_text, doc$text)))\n",
        "      if (length(i_end) > 1) {\n",
        "        i_end = i_end[2]\n",
        "      }\n",
        "\n",
        "      write_log(i_start)\n",
        "      write_log(i_end)\n",
        "\n",
        "      if (length(i_start) != 0 & length(i_end) != 0) {\n",
        "        #i_start = as.numeric(i_start)\n",
        "        #i_end = as.numeric(i_end)\n",
        "        if (i_start < i_end) {        \n",
        "            print(paste0('istart is:',i_start,' iend is:',i_end))\n",
        "            df_txt = doc[i_start:i_end,]\n",
        "            break\n",
        "        }\n",
        "      }\n",
        "\n",
        "    }\n",
        "    if (length(i_start) == 0 || length(i_end) == 0) {\n",
        "      write_log(\"missing section for:\")\n",
        "      write_log(str_href)\n",
        "    }\n",
        "\n",
        "  }\n",
        "  #we could do some text preprocessing here.\n",
        "\n",
        "  df_txt <- as_tibble(df_txt) %>%\n",
        "    #mutate(text = textclean::strip(text)) %>%\n",
        "    mutate(section = str_search)\n",
        "\n",
        "  return(df_txt)\n",
        "}\n",
        "\n",
        "get_document_text <- function(str_ticker, force = FALSE) { #not using force yet\n",
        "  start_time <- Sys.time()\n",
        "\n",
        "  write_log(str_ticker)\n",
        "\n",
        "  str_write_name <- paste0('sec_data_folder/',str_ticker)\n",
        "\n",
        "  write_log(\"get filings links ...\")\n",
        "\n",
        "  filings_csv <- paste0(str_write_name,\"_filings.csv\")\n",
        "  \n",
        "  if (file.exists(filings_csv)) {  #add force equals true\n",
        "    write_log(\"from cache ...\")\n",
        "    \n",
        "    df_filings <- read_csv(filings_csv,col_types = cols()) \n",
        "    df_filings <- df_filings %>% mutate_if(is.logical, as.character)\n",
        "    } else {\n",
        "    write_log(\"from sec ...\")\n",
        "      \n",
        "    df_filings <- get_filings_links(str_ticker) %>%\n",
        "      mutate(ticker = str_ticker) %>%\n",
        "      write_csv(filings_csv)\n",
        "    }\n",
        "\n",
        "  write_log_csv(df_filings)\n",
        "  \n",
        "#for debug\n",
        "  i_test = nrow(df_filings) #for some reason this won't evaulate inside the if statement\n",
        "  if (i_test == 0) {\n",
        "      return(NULL)\n",
        "  }\n",
        "\n",
        "  write_log(\"get section text ...\")\n",
        "\n",
        "  df_data <- (df_filings) %>%\n",
        "    rowwise() %>%\n",
        "    mutate(nest_discussion = map(.x = href, .f = get_mdna_text)) %>%\n",
        "    ungroup() %>%\n",
        "    group_by(period_date) %>%\n",
        "    arrange(desc(period_date))\n",
        "\n",
        "  #jenky - find a rowwise application\n",
        "  a <- df_data %>%\n",
        "    select(period_date,filing_date,type,form_name,documents,nest_discussion) %>%\n",
        "    unnest(nest_discussion)\n",
        "\n",
        "  write_log(\"write to local csv  ...\")\n",
        "  df_data <- a %>%\n",
        "    as_tibble() %>%\n",
        "    write_csv(paste0(str_write_name,\".csv\"))\n",
        "\n",
        "  end_time <- Sys.time()\n",
        "\n",
        "  write_log(end_time - start_time)\n",
        "\n",
        "  return(df_data)\n",
        "}\n",
        "\n",
        "df_tickers <- read_csv('test_ticker_list.csv',col_types = cols()) \n",
        "\n",
        "dir.create('sec_data_folder', showWarnings = FALSE)\n",
        "\n",
        "future::plan(multiprocess)\n",
        "\n",
        "df_data <- future_map_dfr(df_tickers$Symbol, get_document_text,.progress = TRUE) #takes a few minutes\n",
        "#df_data <- map_df(df_tickers$Symbol, get_document_text) # non parallel version\n",
        "print('done')\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "R[write to console]: \n",
            "Attaching package: ‘dplyr’\n",
            "\n",
            "\n",
            "R[write to console]: The following objects are masked from ‘package:stats’:\n",
            "\n",
            "    filter, lag\n",
            "\n",
            "\n",
            "R[write to console]: The following objects are masked from ‘package:base’:\n",
            "\n",
            "    intersect, setdiff, setequal, union\n",
            "\n",
            "\n",
            "R[write to console]: \n",
            "Attaching package: ‘rvest’\n",
            "\n",
            "\n",
            "R[write to console]: The following object is masked from ‘package:purrr’:\n",
            "\n",
            "    pluck\n",
            "\n",
            "\n",
            "R[write to console]: \n",
            "Attaching package: ‘readr’\n",
            "\n",
            "\n",
            "R[write to console]: The following object is masked from ‘package:rvest’:\n",
            "\n",
            "    guess_encoding\n",
            "\n",
            "\n",
            "R[write to console]: Loading required package: future\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[1] \"May_27_2020.log\"\n",
            "[1] \"May_27_2020.csv\"\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "R[write to console]: Error: 'test_ticker_list.csv' does not exist in current working directory ('/content').\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Error: 'test_ticker_list.csv' does not exist in current working directory ('/content').\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1UYcX52F7_pu",
        "colab_type": "code",
        "outputId": "c209ff5d-52d6-4a4d-9c59-8a1ab1696405",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 667
        }
      },
      "source": [
        "#move from R to python\n",
        "\n",
        "!pip install pandarallel\n",
        "\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('vader_lexicon')\n",
        "nltk.download('stopwords')\n",
        "#from transformers import pipeline\n",
        "#nlp = pipeline('sentiment-analysis')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from nltk import tokenize\n",
        "import pandas as pd\n",
        "pd.options.mode.chained_assignment = None\n",
        "import os\n",
        "import pandas as pd\n",
        "import os.path\n",
        "from os import path\n",
        "\n",
        "import numpy as np\n",
        "from pandarallel import pandarallel\n",
        "\n",
        "import time\n",
        "\n",
        "pandarallel.initialize()\n",
        "df_tickers = pd.read_csv('test_ticker_list.csv')\n",
        "\n",
        "LOGFILE = 'sec_nlp_beta.log'\n",
        "f = open(LOGFILE, \"w\")\n",
        "t = time.localtime()\n",
        "current_time = time.strftime(\"%H:%M:%S\", t)\n",
        "f.write(current_time+\": process started\")\n",
        "f.close()\n",
        "\n",
        "FIND_WORDS = ['covid',\n",
        "              'guidance',\n",
        "              'outlook']\n",
        "\n",
        "def check_if_list_found_in_text(text, words=[], return_offset=False, lower_text=True):\n",
        "    result = []\n",
        "    text = (\n",
        "        \" \"\n",
        "        + text.replace(\"_\", \" \")\n",
        "        .replace(\"-\", \" \")\n",
        "        .replace(\",\", \" \")\n",
        "        .replace(\";\", \" \")\n",
        "        .replace('\"', \" \")\n",
        "        .replace(\":\", \" \")\n",
        "        .replace(\".\", \" \")\n",
        "        + \" \"\n",
        "    )\n",
        "    if lower_text:\n",
        "        text = text.lower()\n",
        "    for word in words:\n",
        "        word = (\n",
        "            \" \"\n",
        "            + word.replace(\"_\", \" \")\n",
        "            .replace(\"-\", \" \")\n",
        "            .replace(\",\", \" \")\n",
        "            .replace(\";\", \" \")\n",
        "            .replace('\"', \" \")\n",
        "            .replace(\":\", \" \")\n",
        "            .replace(\".\", \" \")\n",
        "            + \" \"\n",
        "        )\n",
        "        if lower_text:\n",
        "            word = word.lower()\n",
        "        if word in text:\n",
        "            if return_offset:\n",
        "                offset = text.find(word)\n",
        "                # offset = offset if not offset else offset-1\n",
        "                result.append(offset)\n",
        "            else:\n",
        "                result.append(word.strip())\n",
        "    return result\n",
        "\n",
        "def filter_stopwords(sent):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    word_tokens = word_tokenize(sent)\n",
        "    filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
        "    filtered_sentence = []\n",
        "    for w in word_tokens:\n",
        "        if w not in stop_words:\n",
        "            filtered_sentence.append(w)\n",
        "    return ' '.join(filtered_sentence)\n",
        "\n",
        "def sentiment_from_text(sentence):\n",
        "  sentence = filter_stopwords(sentence)\n",
        "  list_found = check_if_list_found_in_text(sentence,FIND_WORDS)\n",
        "  num_found = len(list_found)\n",
        "\n",
        "  ss = sid.polarity_scores(sentence) #NLTK\n",
        "  df = pd.DataFrame.from_dict(ss,orient = \"index\").T\n",
        "  df['transformers_score'] = dict_transformers['score'] #tranformers\n",
        "  df['transformers_label'] = dict_transformers['label']\n",
        "  df['text'] = sentence\n",
        "  df['keywords_found'] = num_found\n",
        "  return pd.concat(dict_sentiment)\n",
        "\n",
        "def filter_stopwords(sent):\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  word_tokens = word_tokenize(sent)\n",
        "  filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
        "  filtered_sentence = []\n",
        "  for w in word_tokens:\n",
        "      if w not in stop_words:\n",
        "          filtered_sentence.append(w)\n",
        "  return ' '.join(filtered_sentence)\n",
        "\n",
        "def df_from_text(text):\n",
        "  sentence_list = tokenize.sent_tokenize(text)\n",
        "  sentence_list\n",
        "  sid = SentimentIntensityAnalyzer()\n",
        "  list_df = []\n",
        "  for sentence in sentence_list:\n",
        "      sentence = filter_stopwords(sentence)\n",
        "      list_found = check_if_list_found_in_text(sentence,FIND_WORDS)\n",
        "      num_found = len(list_found)\n",
        "#if using transformers...\n",
        "#nlp = pipeline('sentiment-analysis')\n",
        "#nlp('We are very happy to include pipeline into the transformers repository.')\n",
        "#>>> {'label': 'POSITIVE', 'score': 0.99893874}\n",
        "      ss = sid.polarity_scores(sentence)\n",
        "      df = pd.DataFrame.from_dict(ss,orient = \"index\").T\n",
        "      df['text'] = sentence\n",
        "      df['keywords_found'] = num_found\n",
        "      list_df.append(df)\n",
        "      return pd.concat(list_df)\n",
        "\n",
        "def py_write_log(str_text):\n",
        "    t = time.localtime()\n",
        "    current_time = time.strftime(\"%H:%M:%S\", t)\n",
        "    print(str_text)\n",
        "    f = open(LOGFILE, \"a\")\n",
        "    f.write(current_time+\": \"+str_text)\n",
        "    f.close()\n",
        "    return\n",
        "\n",
        "def func_sentiment(row):\n",
        "    df = df_from_text(row['text']) #neg neu pos compound text keywords_found\n",
        "    neu = df.iloc[0]['neu']\n",
        "    pos = df.iloc[0]['pos']\n",
        "    neg = df.iloc[0]['neg']\n",
        "    num_rows = 1\n",
        "    compound = df.iloc[0]['compound']\n",
        "    text = df.iloc[0]['text']\n",
        "    keywords_found = df.iloc[0]['keywords_found']\n",
        "    return pd.Series([row['ticker'],row['section'],row['type'],row['period_date'],neu,pos,neg,compound,keywords_found,text,num_rows])\n",
        "\n",
        "master_list_df = []\n",
        "list_tickers = df_tickers['Symbol']\n",
        "#list_tickers = ['MMM']\n",
        "\n",
        "for ticker in list_tickers:\n",
        "    py_write_log(\"working on...\"+ticker)\n",
        "    tic = time.perf_counter()\n",
        "\n",
        "    if path.exists(\"sec_data_folder/\"+ticker+\".csv\"):\n",
        "\n",
        "        df_text = pd.read_csv(\"sec_data_folder/\"+ticker+\".csv\")\n",
        "        if len(df_text) > 0:\n",
        "\n",
        "            df_text['ticker'] = ticker\n",
        "\n",
        "            df_discussion = df_text[df_text['section']=='discussion']\n",
        "\n",
        "            df_out = df_discussion.parallel_apply(func_sentiment, axis=1)\n",
        "            df_out.columns = ['ticker','section','type','period_date','neu','neg','pos','compound','keywords_found','text','num_rows']\n",
        "            #df_out.to_csv(\"test.csv\")\n",
        "            if len(df_out) > 0:\n",
        "\n",
        "                df_out = df_out.groupby(['ticker','period_date','type']).sum().reset_index()\n",
        "                df_out['neg'] = df_out['neg']/df_out['num_rows']\n",
        "                df_out['neu'] = df_out['neu']/df_out['num_rows']\n",
        "                df_out['pos'] = df_out['pos']/df_out['num_rows']\n",
        "                df_out['compound'] = df_out['compound']/df_out['num_rows']\n",
        "\n",
        "                df_error = df_out[df_out['compound'] == 0]\n",
        "                if not df_error.empty:\n",
        "                    py_write_log(\"zero values...\"+ticker)\n",
        "                    df_error.to_csv('sec_nlp_errors.csv',mode = 'a')\n",
        "\n",
        "                df_out.drop(['keywords_found'],axis = 1)\n",
        "                df_out['compound_baseline'] = df_out['compound'] / df_out['compound'].mean()\n",
        "                df_out['neg_baseline'] = df_out['neg'] / df_out['neg'].mean()\n",
        "                df_out['pos_baseline'] = df_out['pos'] / df_out['pos'].mean()\n",
        "                df_out['compound_bdiff'] = df_out['compound_baseline'].diff()\n",
        "                df_out['neg_bdiff'] = df_out['neg_baseline'].diff()\n",
        "                df_out['pos_bdiff'] = df_out['pos_baseline'].diff()\n",
        "                df_out['compound_zscore'] = (df_out['compound'] - df_out['compound'].mean())/df_out['compound'].std(ddof=0)\n",
        "\n",
        "                #always cache\n",
        "                str_score_file = \"sec_data_folder/\"+ticker+\"_score.csv\"\n",
        "                df_out.to_csv(str_score_file)\n",
        "\n",
        "                master_list_df.append(df_out)\n",
        "            else:\n",
        "                py_write_log(\"missing...\"+ticker)\n",
        "        else:\n",
        "            py_write_log(ticker+\" has no data.\")\n",
        "    toc = time.perf_counter()\n",
        "    py_write_log(f\"Text processed in {toc - tic:0.4f} seconds\")\n",
        "\n",
        "if master_list_df:\n",
        "    df_data = pd.concat(master_list_df)\n",
        "    df_data.to_csv('df_data.csv')\n",
        "\n",
        "df = df_data[['period_date','ticker','compound_baseline']]\n",
        "df['quarter_end'] = pd.to_datetime(df['period_date'])\n",
        "df['quarter_end'] = df.quarter_end.map(lambda x: x.strftime('%Y-%m-%d'))\n",
        "\n",
        "#modify odd quarter ends\n",
        "df.loc[df.quarter_end == '2017-04-01', 'quarter_end'] = '2017-03-31'\n",
        "df.loc[df.quarter_end == '2017-07-01', 'quarter_end'] = '2017-06-30'\n",
        "df.loc[df.quarter_end == '2018-04-01', 'quarter_end'] = '2018-03-31'\n",
        "df.loc[df.quarter_end == '2018-07-01', 'quarter_end'] = '2018-06-30'\n",
        "\n",
        "df['quarter_end'] = pd.to_datetime(df['quarter_end'])\n",
        "df['quarter_end'] = df['quarter_end'].dt.to_period('q').dt.end_time #floor at end of quarter\n",
        "df['quarter_end'] = df.quarter_end.map(lambda x: x.strftime('%Y-%m-%d')) #format\n",
        "\n",
        "import numpy as np\n",
        "df_data_pivot = pd.pivot_table(df, values='compound_baseline', index=['quarter_end'],\n",
        "                columns=['ticker'], aggfunc=np.sum, fill_value=0).reset_index()\n",
        "\n",
        "df_data_pivot.to_csv(\"df_data_pivot.csv\")\n",
        "\n",
        "print('done!')\n",
        "\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandarallel in /usr/local/lib/python3.6/dist-packages (1.4.8)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from pandarallel) (0.3.1.1)\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "INFO: Pandarallel will run on 4 workers.\n",
            "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
            "  warnings.warn(\"The twython library has not been installed. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-fbc063ecc628>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mpandarallel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mdf_tickers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test_ticker_list.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mLOGFILE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'sec_nlp_beta.log'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    674\u001b[0m         )\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1891\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File test_ticker_list.csv does not exist: 'test_ticker_list.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iuyztVzxz7uo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#post pivot data to Google Sheet\n",
        "\n",
        "!pip install --upgrade -q gspread\n",
        "\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "import gspread\n",
        "from oauth2client.client import GoogleCredentials as GC\n",
        "gc = gspread.authorize(GC.get_application_default())\n",
        "# create, and save df \n",
        "from gspread_dataframe import set_with_dataframe\n",
        "title = 'sec_nlp_data'\n",
        "gc.create(title)  # if not exist\n",
        "sheet = gc.open(title).sheet1\n",
        "set_with_dataframe(sheet, df_data_pivot) \n",
        "print('done!')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EcrhRus4oJJp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#a sample chart of one ticker.\n",
        "\n",
        "pd.options.mode.chained_assignment = None\n",
        "df_plot = df_data[df_data['ticker'] == 'XOM'] #setting w copy warning\n",
        "\n",
        "df_plot['period_date'] = pd.to_datetime(df_plot['period_date'])\n",
        "df_plot['period_date'] = df_plot.period_date.map(lambda x: x.strftime('%Y-%m-%d')) #format\n",
        "\n",
        "df_plot.compound = pd.to_numeric(df_plot.compound)\n",
        "\n",
        "df_plot.plot.bar(x='period_date', y='compound', rot=90,title='XOM')\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}